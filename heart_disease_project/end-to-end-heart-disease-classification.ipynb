{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Predicting heart disease using machine learning\n",
    "\n",
    "This notebook looks into using various Python based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has heart disease based on their medical attributes \n",
    "\n",
    "We're going to take the following approach: \n",
    "1. Problem definition\n",
    "2. Data\n",
    "3. Evaluation\n",
    "4. Features\n",
    "5. Modelling\n",
    "6. Experimentation\n",
    "\n",
    "## 1. Problem Definition\n",
    "\n",
    "In a statement, \n",
    "> Given clinical parameters about a patient, can we predict whether or not the have heart disease?\n",
    "\n",
    "## 2. Data\n",
    "\n",
    "The original data come from: `https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data`\n",
    "\n",
    "\n",
    "## 3. Evaluation\n",
    "\n",
    "> If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept we'll pursue the project\n",
    "\n",
    "## 4. Features \n",
    "This is where you will get diffrent information about each of the features of your data.\n",
    "\n",
    "**Create Data Dictionary**\n",
    "\n",
    "\n",
    "    1. id (Unique id for each patient)\n",
    "    2. age (Age of the patient in years)\n",
    "    3. origin (place of study)\n",
    "    4. sex (Male/Female)\n",
    "    5. cp chest pain type ([typical angina, atypical angina, non-anginal, asymptomatic])\n",
    "    6. trestbps resting blood pressure (resting blood pressure (in mm Hg on admission to the hospital))\n",
    "    7. chol (serum cholesterol in mg/dl)\n",
    "    8. fbs (if fasting blood sugar > 120 mg/dl)\n",
    "    9. restecg (resting electrocardiographic results)\n",
    "     -- Values: [normal, stt abnormality, lv hypertrophy]\n",
    "    10. thalach: maximum heart rate achieved\n",
    "    11. exang: exercise-induced angina (True/ False)\n",
    "    12. oldpeak: ST depression induced by exercise relative to rest\n",
    "    13. slope: the slope of the peak exercise ST segment\n",
    "    14. ca: number of major vessels (0-3) colored by fluoroscopy\n",
    "    15. thal: [normal; fixed defect; reversible defect]\n",
    "    16. target: the predicted attribute\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Preparing the tools \n",
    "\n",
    "We are going to use pandas, matplotlib and NumPy for data analysis and manipulation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the tools \n",
    "\n",
    "# Regular EDA (exploring data analysis) and plotting libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "# Models from scikit learn \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Model Evaluations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score\n",
    "from sklearn.metrics import RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/heart-disease.csv\")\n",
    "df.shape # rows, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Data Exploration (exploratory data analysis or EDA) \n",
    "\n",
    "The goal here is to find out more about the data and become a aubject matter export on the dataset you are working with.\n",
    "\n",
    "1. What questions are you trying to solve? \n",
    "2. What kind of data do we have and how do we treat diffrent types? \n",
    "3. What's missing from the data and how do you deal with it \n",
    "4. Where are the outliers and why should you care about them? \n",
    "5. How can you add, change or remove features to get more out of your data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find out how many of each class there\n",
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"].value_counts().plot(kind = \"bar\", color=[\"salmon\",\"lightblue\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are there any missing values ? \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Heart Disease Freuquency according to Sex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare target column with sex column \n",
    "pd.crosstab(df.target, df.sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of crosstab \n",
    "pd.crosstab(df.target, df.sex).plot(kind = \"bar\", figsize=(10,6), color=[\"salmon\",\"lightblue\"])\n",
    "\n",
    "plt.title(\"Heart Disease Freuquency for Sex\") \n",
    "plt.xlabel(\"0 = No Disease, 1 = Disease\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.legend([\"Female\",\"Male\"])\n",
    "\n",
    "plt.xticks(rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"thalach\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Age vs Max Heart Rate or Heart Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another figure \n",
    "plt.figure(figsize= (10,6))\n",
    "\n",
    "# Scatter with possitive examples \n",
    "plt.scatter(df.age[df.target ==1],\n",
    "           df.thalach[df.target == 1],\n",
    "           c = \"salmon\");\n",
    "\n",
    "# Scatter with negative examples\n",
    "plt.scatter(df.age[df.target ==0],\n",
    "           df.thalach[df.target == 0],\n",
    "           c = \"lightblue\");\n",
    "\n",
    "plt.legend([\"Heart Disease\",\"No Heart Disease\"]);\n",
    "plt.title(\"Heart Disease in function of Age and Max Heart Rate\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Max Heart Rate\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the age column with a histogram \n",
    "df.age.plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Heart Disease Frequency per Chest Pain Type\n",
    "\n",
    "5. cp chest pain type ([typical angina, atypical angina, non-anginal, asymptomatic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.cp,df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the crosstab more visual\n",
    "\n",
    "pd.crosstab(df.cp,df.target).plot(kind=\"bar\",\n",
    "                                  figsize=(10,6),\n",
    "                                  color= [\"salmon\", \"lightblue\"])\n",
    "\n",
    "# Add some communication \n",
    "plt.title(\"Heart Disease Frequency Per Chest Pain Type\")\n",
    "plt.xlabel(\"Chest Pain Type\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.legend([\"No Disease\", \"Disease\"])\n",
    "plt.xticks(rotation=0); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a correlation matrix \n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make the correlation matrix a little bit prettier\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "fig,ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.heatmap(corr_matrix, \n",
    "                 annot=True,\n",
    "                 linewidths=0.5,\n",
    "                 fmt=\".2f\",\n",
    "                 cmap=\"YlGnBu\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# 5. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "X = df.drop(\"target\",axis = 1) \n",
    "y = df[\"target\"] \n",
    "\n",
    "#reproduce the results \n",
    "np.random.seed(42)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,shuffle=True, test_size = 0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Now we have got our datat split into training and test sets, it's time to build a machine learning model.\n",
    "We will train it (find the patterns) on the training set.\n",
    "And we will test it (use the patterns) on the test set. \n",
    "\n",
    "We are going to try 3 diffrent machine learning models: \n",
    "1. Logistic Regression \n",
    "2. K-Nearest Neighbours Classifier\n",
    "3. Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put models into a Dict \n",
    "\n",
    "models = { \"Logistic Regression\": LogisticRegression(),\n",
    "           \"KNN\": KNeighborsClassifier(),\n",
    "           \"Random Forest\": RandomForestClassifier()}\n",
    "\n",
    "# Create a function to fit and score our models \n",
    "def fit_and_score (models, X_train,X_test,y_train,y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates given machine learning models. \n",
    "    models: a dict of diffrent Scikit.Learn machine learning models \n",
    "    X_train: training data (no labels)\n",
    "    X_test: test data (no labels)\n",
    "    y_train: training data (labels) \n",
    "    y_test: test data (labels) \n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(42) \n",
    "\n",
    "    # Make a dict to keep model scores \n",
    "\n",
    "    model_scores={}\n",
    "\n",
    "    # Loop through models \n",
    "    for name, model in models.items():\n",
    "        #Fit the model to the data \n",
    "        model.fit(X_train,y_train)\n",
    "        #Evaluate the model and append it to the model_scores dict\n",
    "        model_scores[name] = model.score(X_test,y_test)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores = fit_and_score(models,X_train,X_test,y_train,y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare = pd.DataFrame(models_scores, index = [\"accuracy\"])\n",
    "model_compare.T.plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Now we have got a basline model .... and we know a model's first predictions aren't always what we should based our next steps off. What should we do?\n",
    "\n",
    "Let's look at the following: \n",
    "\n",
    "* Hyperparameter tuning\n",
    "* Feature importance\n",
    "* Confusion Matrix\n",
    "* Cross_Validation\n",
    "* Precision\n",
    "* Recall\n",
    "* F1 score\n",
    "* Classification report\n",
    "* ROC curve\n",
    "* Area under the curve (AUC)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (by hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tune KNN\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Create a list of diffrent values for n neighbors\n",
    "neighbors = range (1,21)\n",
    "\n",
    "# Setup KNN instance \n",
    "knn = KNeighborsClassifier();\n",
    "\n",
    "#Loop through diffrent n_neigbors \n",
    "\n",
    "for i in neighbors:\n",
    "    knn.set_params(n_neighbors=i)\n",
    "\n",
    "    # Fit the alg\n",
    "    knn.fit(X_train,y_train)\n",
    "\n",
    "    # Updating the training scores list\n",
    "    train_scores.append(knn.score(X_train,y_train))\n",
    "\n",
    "    # Update the test scores list\n",
    "    test_scores.append(knn.score(X_test,y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neighbors,train_scores, label=\"Train score\")\n",
    "plt.plot(neighbors,test_scores,label=\"Test score\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Model score\")\n",
    "plt.legend();\n",
    "\n",
    "print(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN is not the right algorithm for this problem because the other algorithms perform far better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with RandomizedSearchCV\n",
    "\n",
    "We are going to tune: \n",
    "* LogisticRegression()\n",
    "* RandomForestClassifier()\n",
    "\n",
    "... using RSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hyperparmeter grid for logisitcregression model \n",
    "log_reg_grid={\n",
    "    \"C\":np.logspace(-4,4,20),\n",
    "    \"solver\":[\"liblinear\"]\n",
    "}\n",
    "\n",
    "# Create a HP grid fpr RandomForestClass.\n",
    "rf_grid ={\n",
    "    \"n_estimators\":np.arange(10,1000,50),\n",
    "    \"max_depth\":[None,3,5,10],\n",
    "    \"min_samples_split\": np.arange(2,20,2),\n",
    "    \"min_samples_leaf\":np.arange(1,20,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Now we have got hyperparameters grids for each of our models, let's tune them using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune LR Model\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for LogisticRegression\n",
    "rs_log_reg = RandomizedSearchCV(LogisticRegression(),\n",
    "                                param_distributions=log_reg_grid,\n",
    "                                cv=5,\n",
    "                                n_iter = 20,\n",
    "                                verbose = True)\n",
    "\n",
    "# Fit random hyperparameter search model for LogisticRegression\n",
    "rs_log_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_log_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Now we have tuned LogisticRegression(), let's do the same for RandomForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for RandomForestClassifier\n",
    "rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n",
    "                                param_distributions=rf_grid,\n",
    "                                cv=5,\n",
    "                                n_iter = 20,\n",
    "                                verbose = True)\n",
    "\n",
    "# Fit random hyperparameter search model for LogisticRegression\n",
    "rs_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_rf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with GridSearchCV \n",
    "\n",
    "Since our LogisticRegression model provides the best scores so far, we will try and improve the HP by GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffrent hyperparameters for our LogisticRegression Model\n",
    "log_reg_grid={\n",
    "    \"C\":np.logspace(-4,4,30),\n",
    "    \"solver\":[\"liblinear\"]\n",
    "}\n",
    "\n",
    "# Setup grid hyperparameter search for LogisticRegression\n",
    "gs_log_reg = GridSearchCV(LogisticRegression(),\n",
    "                          param_grid=log_reg_grid,\n",
    "                          cv = 5,\n",
    "                          verbose = True)\n",
    "# Fit the model\n",
    "gs_log_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_log_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "## Evaluating our tuned machine learning classifier, beyond accuracy \n",
    "\n",
    "* ROC Curve and AUC Score\n",
    "* Confusuin Matrix\n",
    "* Classification Report\n",
    "* Precision\n",
    "* Recall \n",
    "* F1-Score\n",
    "\n",
    "... and it would be great if cross-validation was used where possible \n",
    "\n",
    "To make compariosons and evaluate our trained model, first we need to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions with tuned model\n",
    "y_preds = gs_log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_score = gs_log_reg.predict_proba(X_test)[:, 1]   # Wahrscheinlichkeit f√ºr Klasse 1\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_score)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "disp = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_log_reg.best_params_\n",
    "\n",
    "def plot_conf_mat(y_test, y_preds):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cbar=False)\n",
    "    plt.xlabel(\"Predicted label\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True label\") # true labels go on the y-axis \n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Now we've got a ROC curve, an AUC metric and a confusion matrix, let's get a classification report as well as cross-validated precision, recall and f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Calculating evaluation metrics using cross-validation\n",
    "\n",
    "We're going to calculate precision, recall and f1-score of our model using cross-validation and to do so we'll be using `cross_cal_score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check best Hyperparameters \n",
    "gs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new classifier with best parameters\n",
    "clf = LogisticRegression(C= 0.20433597178569418, solver = \"liblinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated accuracy\n",
    "cv_acc = cross_val_score(clf,X,y,cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "cv_acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validated precision\n",
    "cv_prec = cross_val_score(clf,X,y,cv = 5, scoring = \"precision\")\n",
    "cv_prec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validated recall\n",
    "cv_rec = cross_val_score(clf,X,y,cv = 5, scoring = \"recall\")\n",
    "cv_rec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validates f1-score\n",
    "cv_f1 = cross_val_score(clf,X,y,cv = 5, scoring = \"f1\")\n",
    "cv_f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validated metrics \n",
    "cv_metrics = pd.DataFrame({\"Accuracy\": cv_acc.mean(),\n",
    "                           \"Precision\":cv_prec.mean(),\n",
    "                           \"Recall\":cv_rec.mean(),\n",
    "                           \"F1\":cv_f1.mean()},\n",
    "                          index = [0])\n",
    "cv_metrics.T.plot.bar(title=\"Cross-validated classification metrics\", legend= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "### Feature Importance \n",
    "\n",
    "Feature importance is another way of asking which features contributed most to tthe outcomes of the model and how did the contribute \n",
    "\n",
    "Finding feature importance is diffrent do each machine learning model. \n",
    "\n",
    "Let's find the feature importance for our LR model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an instance of Logistic Regression\n",
    "# Create a new classifier with best parameters\n",
    "clf = LogisticRegression(C= 0.20433597178569418, solver = \"liblinear\")\n",
    "\n",
    "clf.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check coef_\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = dict(zip(df.columns,list(clf.coef_[0])))\n",
    "feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Feature Importance \n",
    "feature_df = pd.DataFrame(feature_dict, index =[0])\n",
    "feature_df.T.plot.bar(title=\"Feature importance\", legend = False); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"sex\"],df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"slope\"],df[\"target\"]) #  slope: the slope of the peak exercise ST segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "## 6. Experimenations\n",
    "\n",
    "If you haven't hit your evaluation metric yet... ask yourself...\n",
    "\n",
    "* Could you collect more data?\n",
    "* Could you try a better model? Like CatBoost or XGBoost?\n",
    "* Could you improve the current models? (beyond what we've done so far)\n",
    "* If your model is good enough ( you have hit your evaluation metric)\n",
    "* How would you export it and share it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
