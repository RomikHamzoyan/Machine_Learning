{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-learn (sklearn)\n",
    "\n",
    "This notebook demonstrates some of the most useful functions of the beautiful Scikit-Learn library.\n",
    "\n",
    "what we're going to cover: \n",
    "\n",
    "0. An end-to-end Scikit Learn workflow\n",
    "1. Getting the data ready\n",
    "2. choose the right estimator/algorithm for our problems\n",
    "3. Fit the model/algorithm and use it to make predictions on our data\n",
    "4. Evaluating a model\n",
    "5. Improve a model\n",
    "6. Save and load a trained model\n",
    "7. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 0. An end-to-end Scikit-Learn workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the data ready \n",
    "heart_disease = pd.read_csv(\"./data/heart-disease.csv\")\n",
    "heart_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X (features matrix)\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "\n",
    "#Create Y (Labels)\n",
    "Y = heart_disease[\"target\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Choose the right model and hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100) # no need of the oarameter\n",
    "\n",
    "# We'll keep the default hyperparameters \n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fit the model to the training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction \n",
    "y_preds = clf.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the model \n",
    "clf.score(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_Test, Y_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(Y_Test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_Test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_Test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Improve a model\n",
    "# Try diffrent amount of n_estimators\n",
    "np.random.seed(42)\n",
    "for i in range (10,100,10):\n",
    "    print (f\"Trying model with {i} estimators...\")\n",
    "    clf = RandomForestClassifier(n_estimators=i).fit(X_Train,Y_Train)\n",
    "    print(f\"Model accuracy on test set: {clf.score(X_Test,Y_Test)*100:.2f}%\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save a model and load it \n",
    "import pickle\n",
    "\n",
    "with open(\"random_forest_model_1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"random_forest_model_1.pkl\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "    \n",
    "loaded_model.score(X_Test, Y_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's listify the contents \n",
    "\n",
    "whats_were_covering = [\n",
    "\"0. An end-to-end Scikit Learn workflow\",\n",
    "\"1. Getting the data ready\",\n",
    "\"2. choose the right estimator/algorithm for our problems\",\n",
    "\"3. Fit the model/algorithm and use it to make predictions on our data\",\n",
    "\"4. Evaluating a model\",\n",
    "\"5. Improve a model\",\n",
    "\"6. Save and load a trained model\",\n",
    "\"7. Putting it all together\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "whats_were_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# 1. Getting the data ready\n",
    "Three main things we have to do. \n",
    "1. Split the data into features and labels  ( X & y)\n",
    "2. Filling (also called imputing) or disregarding missing values\n",
    "3. Vpnverting non-numerical values to numerical values (also called feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = heart_disease[\"target\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 1.1 Make sure it's all numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales = pd.read_csv (\"./data/car-sales-extended.csv\")\n",
    "car_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into X and y\n",
    "X = car_sales.drop(\"Price\", axis = 1)\n",
    "y = car_sales[\"Price\"]\n",
    "\n",
    "# Split into Training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ML model\n",
    "from sklearn.ensemble import RandomForestRegressor # Predict a number\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "# model.fit(X_train, y_train) # throw a error because the model cant handle strings\n",
    "# model.score(X_Test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder # Turn the categories into numbers \n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\",\"Doors\"] # Doors because there a only 3 typen (5,4 and 3) it is also categorical\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\", \n",
    "                                 one_hot,\n",
    "                                 categorical_features)],\n",
    "                               remainder = \"passthrough\")\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "\n",
    "transformed_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed_X) # the odometer ha not changed because it is not categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "<img src=\"./images/one_hot.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING: One-Hot-Encoding for categorical columns (and passthrough for numeric columns)\n",
    "#\n",
    "# Motivation:\n",
    "# Most ML algorithms in scikit-learn expect purely numerical input (a matrix in R^n).\n",
    "# Raw categorical values such as \"Toyota\" or \"Red\" cannot be used directly.\n",
    "# Also, we must NOT simply map categories to integers (e.g., Toyota=1, BMW=2),\n",
    "# because that would create an artificial order and distance between categories.\n",
    "#\n",
    "# OneHotEncoder:\n",
    "# - Learns the set of unique categories in each selected column during `fit()`.\n",
    "# - During `transform()`, it converts each category into a binary vector:\n",
    "#   Example for Colour = {Red, Blue, Black}:\n",
    "#     Red   -> [1, 0, 0]\n",
    "#     Blue  -> [0, 1, 0]\n",
    "#     Black -> [0, 0, 1]\n",
    "# - This produces \"orthogonal\" features (no implied ranking), which is the correct\n",
    "#   mathematical representation for nominal variables.\n",
    "# - By default, the output is a sparse matrix to save memory (because most entries are 0).\n",
    "#\n",
    "# Why \"Doors\" is treated as categorical:\n",
    "# Even though Doors is numeric-looking (3, 4, 5), the values represent discrete types,\n",
    "# not a continuous measurement. The difference between 3 and 4 doors is not a linear\n",
    "# quantity in the same way as \"Odometer (KM)\". Treating it as categorical avoids\n",
    "# misleading linear assumptions (especially important for linear models).\n",
    "#\n",
    "# ColumnTransformer:\n",
    "# - Applies transformations to specific columns only.\n",
    "# - Here: apply OneHotEncoder to [\"Make\", \"Colour\", \"Doors\"].\n",
    "# - `remainder=\"passthrough\"` ensures that all other columns (e.g., Odometer, Price, etc.)\n",
    "#   remain in the dataset unchanged. Without this, those columns would be dropped.\n",
    "#\n",
    "# fit_transform(X):\n",
    "# - `fit`: discovers categories in the selected columns (learns the encoding schema).\n",
    "# - `transform`: outputs a final numerical design matrix that concatenates:\n",
    "#     (a) one-hot encoded categorical columns\n",
    "#     (b) untouched numerical columns (passthrough)\n",
    "# - The resulting matrix `transformed_X` is ready for model training in scikit-learn.\n",
    "#\n",
    "# Best practice note:\n",
    "# In a full ML workflow, you typically fit the transformer ONLY on training data\n",
    "# (fit on X_train, transform X_train and X_test) to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way \n",
    "dummies = pd.get_dummies(car_sales[[\"Make\", \"Colour\",\"Doors\"]]) # dont work in int columns\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's refit the model \n",
    "np.random.seed(42)\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(transformed_X,y,test_size = 0.2)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test) # to less data try it in real life"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### 1.2 What if there were missing values? \n",
    "\n",
    "1. Fill them with some value (also known as imputation).\n",
    "2. Remove the samples with missing data altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import car_sales missing data\n",
    "car_sales_missing = pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum() #how many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X & y\n",
    "X = car_sales_missing.drop(\"Price\", axis = 1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try and convert our data to numbers\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder # Turn the categories into numbers \n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\",\"Doors\"] # Doors because there a only 3 typen (5,4 and 3) it is also categorical\n",
    "one_hot = OneHotEncoder(sparse_output=False) # getting a array not a sparse Matrix\n",
    "transformer = ColumnTransformer([(\"one_hot\", \n",
    "                                 one_hot,\n",
    "                                 categorical_features)],\n",
    "                               remainder = \"passthrough\")\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed_X).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Standardization (Z-Score Scaling)\n",
    "# This transforms each numerical feature so that it has a mean of 0\n",
    "# and a standard deviation of 1.\n",
    "# The resulting values can be negative or positive and are not bounded.\n",
    "# Standardization is commonly used for distance-based or gradient-based\n",
    "# algorithms such as Logistic Regression, SVMs, and KNN.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Beispiel-Datensatz\n",
    "data = {\n",
    "    \"Odometer_KM\": [6000, 45000, 120000, 250000, 345000],\n",
    "    \"Repair_Cost\": [100, 300, 700, 1200, 1700]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialisiere StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit + Transform\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df),\n",
    "    columns=df.columns\n",
    ")\n",
    "\n",
    "df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Normalization (Min-Max Scaling)\n",
    "# This rescales each numerical feature to a fixed range between 0 and 1.\n",
    "# The minimum value of a feature becomes 0, the maximum value becomes 1,\n",
    "# and all other values are scaled proportionally in between.\n",
    "# Min-Max scaling is often used when features have known bounds or when\n",
    "# training neural networks, where a consistent input range is beneficial.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Beispiel-Datensatz\n",
    "data = {\n",
    "    \"Odometer_KM\": [6000, 45000, 120000, 250000, 345000],\n",
    "    \"Repair_Cost\": [100, 300, 700, 1200, 1700]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialisiere Min-Max-Scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit + Transform (nur für Demonstration an Gesamtdaten)\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df),\n",
    "    columns=df.columns\n",
    ")\n",
    "\n",
    "df_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Option 2: Fill missing values with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing = pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with no labels\n",
    "car_sales_missing.dropna(subset = [\"Price\"], inplace = True)\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X & y\n",
    "X = car_sales_missing.drop(\"Price\", axis = 1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with scikit Learn (better way before Encoding into numbers)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Fill categorical values with \"missing\" & numerical values with mean \n",
    "cat_imputer = SimpleImputer(strategy=\"constant\", fill_value = \"missing\")\n",
    "door_imputer = SimpleImputer ( strategy = \"constant\", fill_value = 4) \n",
    "num_imputer = SimpleImputer(strategy = \"mean\")\n",
    "\n",
    "#Define columns \n",
    "cat_features = [\"Make\",\"Colour\"]\n",
    "door_features = [\"Doors\"]\n",
    "num_features = [\"Odometer (KM)\"]\n",
    "\n",
    "#Create a Imputer (something that fills missing data) \n",
    "imputer = ColumnTransformer([\n",
    "    (\"cat_imputer\", cat_imputer, cat_features),\n",
    "    (\"door_imputer\",door_imputer,door_features),\n",
    "    (\"num:imputer\", num_imputer, num_features)\n",
    "])\n",
    "\n",
    "#Transform the data \n",
    "filled_X = imputer.fit_transform(X)\n",
    "filled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_filled = pd.DataFrame(filled_X, columns=[\"Make\",\"Colour\",\"Doors\",\"Odometer (KM)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X & y don't change\n",
    "X = car_sales_filled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try and convert our data to numbers\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder # Turn the categories into numbers \n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\",\"Doors\"] # Doors because there a only 3 typen (5,4 and 3) it is also categorical\n",
    "one_hot = OneHotEncoder(sparse_output=False) # getting a array not a sparse Matrix\n",
    "transformer = ColumnTransformer([(\"one_hot\", \n",
    "                                 one_hot,\n",
    "                                 categorical_features)],\n",
    "                               remainder = \"passthrough\") # dont change other columns\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we've got our data as numbers and filled ( no missing data) \n",
    "#Let's fit a model \n",
    "np.random.seed(42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test) # worse score maybe wrong model (or we need more samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "whats_were_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "# 2. choose the right estimator/algorithm for our problems\n",
    "\n",
    "Some things to note: \n",
    "* Sklearn refers to machine learning models, algorithms as estimators. \n",
    "* Classification problem - predicting a category ( heart disease or not)\n",
    "* Sometime you'll see clf (short fpr classifier) used as a classification estimator\n",
    "* Regression problem - predicting a number (selling price for a car) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "<img src =\"./images/sklearn-ml-map.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### 2.1 picking a ML model for regression problem \n",
    "Let's use  the california housing toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.DataFrame(df[\"data\"])\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.DataFrame(df[\"data\"], columns = df[\"feature_names\"])\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"target\"]= df [\"target\"]\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = housing_df.drop(\"target\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import algorithm\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.model_selection import train_test_split\n",
    "#setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data \n",
    "X = housing_df\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Split into train and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# Instantiate and fit the model \n",
    "model = Ridge()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varianz beschreibt die vertikale Streuung der Zielwerte um einen Referenzwert.\n",
    "# \n",
    "# - Gesamtvarianz (SST):\n",
    "#   Abstand der echten Werte y zum Mittelwert y_mean.\n",
    "#   Sie zeigt, wie stark die Daten insgesamt streuen,\n",
    "#   also wie viel \"Information\" bzw. Unordnung in den Zielwerten steckt.\n",
    "#\n",
    "# - Nicht erklärte Varianz (SSE):\n",
    "#   Abstand der echten Werte y zu den Modellvorhersagen y_pred.\n",
    "#   Sie zeigt, wie viel dieser ursprünglichen Streuung nach dem Modell\n",
    "#   noch übrig bleibt und vom Modell nicht erklärt werden kann.\n",
    "#\n",
    "# - Zusammenhang zu R²:\n",
    "#   R² = 1 - (nicht erklärte Varianz / Gesamtvarianz)\n",
    "#   → R² misst, welcher Anteil der ursprünglichen Streuung\n",
    "#     durch das Modell reduziert bzw. erklärt wurde.\n",
    "#\n",
    "# Grafische Vorstellung:\n",
    "#   • Abstand Punkt → Mittelwert  = Gesamtvarianz\n",
    "#   • Abstand Punkt → Modelllinie = nicht erklärte Varianz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "What if Ridge didn't work or the score didn't fit our needs \n",
    "\n",
    "Try a Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble-Methoden kombinieren mehrere einzelne Modelle (Base Learners)\n",
    "# zu einem gemeinsamen Gesamtmodell, um stabilere und genauere Vorhersagen\n",
    "# zu erzielen als mit einem einzelnen Modell.\n",
    "#\n",
    "# Motivation:\n",
    "# Einzelmodelle leiden häufig unter hohem Bias (Underfitting) oder\n",
    "# hoher Varianz (Overfitting). Ensembles wirken diesem Problem entgegen.\n",
    "#\n",
    "# Grundprinzip:\n",
    "# - Mehrere Modelle werden trainiert\n",
    "# - Jedes Modell macht leicht unterschiedliche Fehler\n",
    "# - Durch Mittelung oder Gewichtung heben sich Fehler teilweise auf\n",
    "#\n",
    "# Hauptarten von Ensemble-Methoden:\n",
    "#\n",
    "# 1) Bagging (z. B. Random Forest):\n",
    "#    - Modelle werden parallel auf zufälligen Stichproben trainiert\n",
    "#    - Reduziert hauptsächlich die Varianz\n",
    "#    - Besonders effektiv bei instabilen Modellen (z. B. Entscheidungsbäume)\n",
    "#\n",
    "# 2) Boosting (z. B. Gradient Boosting, XGBoost):\n",
    "#    - Modelle werden sequenziell trainiert\n",
    "#    - Jedes neue Modell fokussiert sich auf die Fehler der vorherigen\n",
    "#    - Reduziert hauptsächlich den Bias\n",
    "#\n",
    "# 3) Stacking:\n",
    "#    - Mehrere unterschiedliche Modelle werden kombiniert\n",
    "#    - Ein Meta-Modell lernt, wie die Vorhersagen optimal gemischt werden\n",
    "#\n",
    "# Zusammenhang zu Bias–Varianz-Tradeoff:\n",
    "# Ensemble-Methoden reduzieren Varianz, Bias oder beides und senken dadurch\n",
    "# die nicht erklärte Varianz, was zu besserer Generalisierung führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the RandomForestRegressor model class\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Setup random seed \n",
    "np.random.seed(42)\n",
    "\n",
    "# model \n",
    "rfg = RandomForestRegressor()\n",
    "\n",
    "# data \n",
    "# Create the data \n",
    "X = housing_df\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Split into train and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "rfg.fit(X_train, y_train) \n",
    "\n",
    "rfg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "## 2.2 Picking a ML model for a classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = pd.read_csv(\"data/heart-disease.csv\")\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the LinearSVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Setup random seed \n",
    "np.random.seed(42)\n",
    "\n",
    "# getting data ready \n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "#Instantiate LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#Evaluate the Linear SVC\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Works good! LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed \n",
    "np.random.seed(42)\n",
    "\n",
    "# getting data ready \n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "#Instantiate RandomForest\n",
    "clf = RandomForestClassifier()\n",
    "#fit the model to the data (training)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#Evaluate the Random Forest ( use the patterns the model has learned)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "whats_were_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "## 3. Fit the model/algorithm and use it to make predictions on our data\n",
    "\n",
    "### 3.1 Fitting a model to the data\n",
    "\n",
    "X = features, features variables, data\n",
    "\n",
    "y = labels , targets, target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed \n",
    "np.random.seed(42)\n",
    "\n",
    "# getting data ready \n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "#Instantiate RandomForest\n",
    "clf = RandomForestClassifier()\n",
    "#fit the model to the data (training)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#Evaluate the Random Forest ( use the patterns the model has learned)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fit method find patterns when the target is 1 and when target is 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "### 3.2 Make predictions using a ML model\n",
    "\n",
    "2 ways to make predictions\n",
    "\n",
    "1. predict()\n",
    "2. predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a trained model to make predictions with predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.predict(np.array[1,6,7,8,5])# this doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare predictions to truth labels to eval. the model\n",
    "y_preds = clf.predict(X_test)\n",
    "np.mean(y_preds == y_test) # this is what the method score does the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way for the accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_preds,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "Make predictions with predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba returns probabilities of a classification label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets predict on the same data\n",
    "clf.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "predict () can also be used for regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df\n",
    "y = y = pd.Series(fetch_california_housing()[\"target\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make a predictions\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COmpare the predictions to the truth \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test,y_preds)\n",
    "\n",
    "# Mean Absolute Error (MAE):\n",
    "# Der MAE gibt an, um wie viele Einheiten ein Modell im Durchschnitt danebenliegt.\n",
    "# Dazu wird für jede Vorhersage die absolute Abweichung zum echten Wert berechnet\n",
    "# (Minuszeichen werden ignoriert) und anschließend der Mittelwert dieser Abweichungen gebildet.\n",
    "# Der MAE hat dieselbe Einheit wie die Zielvariable und ist leicht interpretierbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "whats_were_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "## 4. Evaluating a model\n",
    "\n",
    "three ways to evaluate Scikit-Learn models/estimators:\n",
    "\n",
    "    1. Estimator's built-in score method\n",
    "    2. the scoring parameter \n",
    "    3. Problem specific metric functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "### 4.1 Evaluating a model with the score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X & y\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "#Create train test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)\n",
    "\n",
    "#Create Model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Fit model\n",
    "rfc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(X_train,y_train) # the highest vakue for the score method is 1.0 and lowest is 0.0 the return value is the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "Let's use the data on a regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X & y\n",
    "X = pd.DataFrame(fetch_california_housing()[\"data\"], columns = fetch_california_housing()[\"feature_names\"])\n",
    "y = fetch_california_housing()[\"target\"]\n",
    "\n",
    "#Create train test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)\n",
    "\n",
    "#Create Model\n",
    "rfc = RandomForestRegressor()\n",
    "\n",
    "# Fit model\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(X_test,y_test) # the default score (evaluation metric is r_squared for regression algorithms highest = 1.0, lowest 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "### 4.2 Evaluating a model using the scoring parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X & y\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "#Create train test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)\n",
    "\n",
    "#Create Model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Fit model\n",
    "rfc.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "<img src=\"./images/cross_validation.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf,X,y, cv = 5) # returns a array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "#SIngle training and test split score\n",
    "clf_single_score = clf.score(X_test,y_test) \n",
    "\n",
    "# take the mean of 5fold cv score \n",
    "clf_cross_val_score = np.mean (cross_val_score(clf,X,y))\n",
    "\n",
    "#compare the two \n",
    "clf_single_score, clf_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring parameter set to None by default \n",
    "cross_val_score(clf,X,y,scoring = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation wird verwendet, um die tatsächliche Leistungsfähigkeit eines Modells\n",
    "# zuverlässig zu bewerten. Im Gegensatz zu einem einzelnen Train/Test-Split mittelt sie\n",
    "# die Performance über mehrere Datenaufteilungen und reduziert damit Zufallseinflüsse.\n",
    "# Sie wird standardmäßig bei Modellvergleich, Hyperparameter-Tuning und in professionellen\n",
    "# ML-Projekten eingesetzt, während ein Single Split nur für schnelle Tests geeignet ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "### 4.2 Classification model evaluation metrics \n",
    "\n",
    "1. Accuracy\n",
    "2. Area under ROC Curve\n",
    "3. Confusion matrix\n",
    "4. Classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed (42)\n",
    "X = heart_disease.drop(\"target\",axis=1)\n",
    "y =  heart_disease[\"target\"]\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "cross_val_score(clf,X,y,cv =5) # return value => mean accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score = cross_val_score(clf,X,y,cv =5)\n",
    "np.mean(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Heart Disease Classifier Cross-Validated Accuracy: {np.mean(cross_val_score)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "### Area under the reciever operating charateristic curve ( AUC/ROC )\n",
    "\n",
    "* Area under curve (AUC)\n",
    "* ROC\n",
    "  \n",
    "ROC curves are a comparison of a model's true positive rate (tpr) versus a model false positive rate (fpr)\n",
    "\n",
    "* True positive = model predicts 1 when truth is 1\n",
    "* False positive = model predicts 1 when truth is 0\n",
    "* True negative = model predicts 0 when truth is 0\n",
    "* False negative = model predicts 0 when truth is 1\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Make Predictions with probalilities \n",
    "y_probs = clf.predict_proba(X_test)\n",
    "y_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_positive = y_probs[:,1]\n",
    "y_probs_positive[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fpr, tpr and tresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)\n",
    "\n",
    "# Check the false positive rates\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for plotting ROC curves \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve (fpr, tpr):\n",
    "    \"\"\"\n",
    "    Plots a ROC Curve give the fpr and tpr of a model\n",
    "    \"\"\"\n",
    "    #plot the curve \n",
    "    plt.plot (fpr,tpr, color=\"orange\", label=\"ROC\")\n",
    "    #plot line with no predictive power (baseline) \n",
    "    plt.plot([0,1],[0,1],color=\"darkblue\", linestyle = \"--\", label= \"Guessing\") \n",
    "\n",
    "    #Customizing the plot\n",
    "    plt.xlabel(\"False positive rate (fpr)\")\n",
    "    plt.ylabel(\"True positive rate (tpr)\")\n",
    "    plt.title (\"Reciever Operating Characteristic (ROC) Curve\") \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test,y_probs_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perfect ROC and AUC score \n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_test)\n",
    "plot_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perfect AUC score \n",
    "roc_auc_score(y_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ROC & AUC – Kurzfassung\n",
    "# =============================================================================\n",
    "#\n",
    "# ROC (Receiver Operating Characteristic):\n",
    "# Zeigt, wie sich ein Klassifikationsmodell bei allen möglichen Schwellenwerten\n",
    "# verhält. Dargestellt wird der Zusammenhang zwischen:\n",
    "# - True Positive Rate  (wie viele echte Positive erkannt werden)\n",
    "# - False Positive Rate (wie viele Negative fälschlich als positiv erkannt werden)\n",
    "#\n",
    "# AUC (Area Under the Curve):\n",
    "# Eine einzelne Zahl, die die ROC-Kurve zusammenfasst.\n",
    "# Sie misst die Trennfähigkeit des Modells.\n",
    "#\n",
    "# Intuition:\n",
    "# AUC = Wahrscheinlichkeit, dass ein zufällig positives Beispiel\n",
    "# einen höheren Score erhält als ein zufällig negatives.\n",
    "#\n",
    "# Interpretation:\n",
    "# AUC = 1.0 -> perfekte Trennung\n",
    "# AUC = 0.5 -> Zufall\n",
    "#\n",
    "# Merksatz:\n",
    "# ROC/AUC bewerten nicht eine konkrete Entscheidung,\n",
    "# sondern wie gut das Modell positive von negativen Fällen trennt.\n",
    "# ========================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A Confusion Matrix is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict. \n",
    "In essence, giving you an idea of where the model is getting confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix with pd.crosstab\n",
    "\n",
    "pd.crosstab(y_test,y_preds, rownames = [\"Actual Labels\"], colnames = [\"Predicted Labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "24+8+3+26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    "<img src=\"./images/cm_anatomy.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  # How to install a new package in JN \n",
    "#!conda install --yes --prefix {sys.prefix} seaborn  already installed in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our Confusion matrix more visual with seaborn heatmap()\n",
    "import seaborn as sns\n",
    "\n",
    "#set the font scale \n",
    "sns.set(font_scale=1.5)\n",
    "#Create a confusion matrix \n",
    "conf_mat = confusion_matrix(y_test,y_preds)\n",
    "\n",
    "# Plot is using Seaborn\n",
    "sns.heatmap(conf_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "### Creating a confusion matrix using scikit Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator = clf, X = X, y= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true = y_test, y_pred= y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "### Classification Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "\n",
    "print(classification_report(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "<img src=\"./images/cr_anatomy.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where precision and recall become vlauable \n",
    "\n",
    "disease_true = np.zeros(10000)\n",
    "disease_true[0] = 1 # only one possitive\n",
    "\n",
    "disease_preds = np.zeros(10000) # model predictions \n",
    "\n",
    "pd.DataFrame(classification_report(disease_true,disease_preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "* Accuray is a good measure to start with if all classes are balanced (e.g same amount of samples which are labelled with 0 or 1)\n",
    "* Precision and recall become more important when classes are imbalanced.\n",
    "* If false positive predictions are worse than false negatives, aim for higer precision\n",
    "* If false negative predictions are worse than false positives, aim for higher recall.\n",
    "* f1-score is a combination of precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158",
   "metadata": {},
   "source": [
    "### 4.2.2 Regression model evaluation metrics \n",
    "\n",
    "Model evaluation metrics documentation - https://scikit-learn.org/0.15/modules/model_evaluation.html#regression-metrics\n",
    "\n",
    "The ones we are going to cover are: \n",
    "\n",
    "1. R^2 or coefficient of determination \n",
    "2. Mean Absolute Error MAE\n",
    "3. Mean squared Error MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df\n",
    "y = df[\"target\"]\n",
    "\n",
    "X_train,X_test, y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Fill an array with y_test mean\n",
    "y_test_mean = np.full(len(y_test),y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_mean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true = y_test, \n",
    "        y_pred = y_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true = y_test, \n",
    "        y_pred = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167",
   "metadata": {},
   "source": [
    "### Mean absolute error\n",
    "\n",
    "MAE is the average of the absolute diffrences between predictions and actual values \n",
    "It gives you an idea of how wrong your model predictions are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_preds)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame( data = {\"actual values\": y_test, \"predicted values\": y_preds})\n",
    "df2[\"diffrences\"]= df2[\"predicted values\"] - df2[\"actual values\"]\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE using formulas and diffrences \n",
    "np.abs(df2[\"diffrences\"]).mean() # MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "### Mean squared error \n",
    "\n",
    "MSE is the mean of the square of the errors between actual and predictied values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared Error \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_preds)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"squared_diffrences\"] = np.square(df2[\"diffrences\"])\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE by hand \n",
    "squared = np.square(df2[\"diffrences\"])\n",
    "squared.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error = df2.copy()\n",
    "df_large_error.loc[0, \"squared_diffrences\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE with large error \n",
    "df_large_error[\"squared_diffrences\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error.loc[1:100,\"squared_diffrences\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE with large error \n",
    "df_large_error[\"squared_diffrences\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183",
   "metadata": {},
   "source": [
    "<img src=\"./images/regression_metrics.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184",
   "metadata": {},
   "source": [
    "### 4.2.3 Finally using the scoring parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\",axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "clf = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Cross Validation accuracy \n",
    "\n",
    "cv_acc = cross_val_score(clf, X,y, cv = 5, scoring = None) # if scoring = None, estimators default scoring evaluation metric is used (accuracy for classification problem)\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated accuracy \n",
    "print(f\"The cross-calidated accuracy is: {np.mean(cv_acc)*100:2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cv_acc = cross_val_score(clf, X,y, cv = 5, scoring = \"accuracy\")\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated accuracy \n",
    "print(f\"The cross-calidated accuracy is: {np.mean(cv_acc)*100:2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n",
    "np.random.seed(42)\n",
    "cv_precision = cross_val_score(clf, X,y, cv = 5, scoring = \"precision\")\n",
    "cv_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated precision\n",
    "print(f\"The cross-calidated precision is: {np.mean(cv_precision)*100:2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall \n",
    "np.random.seed(42)\n",
    "cv_recall = cross_val_score(clf, X,y, cv = 5, scoring = \"recall\")\n",
    "cv_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated recall\n",
    "print(f\"The cross-calidated recall is: {np.mean(cv_recall)*100:2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194",
   "metadata": {},
   "source": [
    "Let's see the scoring parameter bein used for a regression problem...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df\n",
    "y = df[\"target\"]\n",
    "\n",
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cv_r2 = cross_val_score(model,X,y, cv = 3, scoring = None)\n",
    "np.mean(cv_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE \n",
    "cv_mae = cross_val_score(model,X,y, cv = 3, scoring = \"neg_mean_absolute_error\")\n",
    "np.mean(cv_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE\n",
    "cv_mse = cross_val_score(model,X,y, cv = 3, scoring = \"neg_mean_squared_error\")\n",
    "np.mean(cv_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201",
   "metadata": {},
   "source": [
    "### 4.3 Using diffrent evaluation metrics as Scikit-Learn functions \n",
    "\n",
    "The 3rd way to evaluate scikit learn machine learning models is to using sklearn.metrics module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score,recall_score,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X and y \n",
    "X = heart_disease.drop(\"target\",axis = 1) \n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split \n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2) \n",
    "\n",
    "# Create model \n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test,y_preds), precision_score(y_test,y_preds), recall_score(y_test,y_preds), f1_score(y_test,y_preds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X and y \n",
    "X = housing_df\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Split \n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2) \n",
    "\n",
    "# Create model \n",
    "clf = RandomForestRegressor()\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "r2_score(y_test,y_preds), mean_absolute_error(y_test,y_preds), mean_squared_error(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {},
   "outputs": [],
   "source": [
    "whats_were_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207",
   "metadata": {},
   "source": [
    "## 5. Improve a model\n",
    "\n",
    "First predicitions = baseline predictions.\n",
    "First model = baseline model. \n",
    "\n",
    "From a data perspective: \n",
    "* Could we collect more data? (the more data, the better)\n",
    "* Could we improve our data?\n",
    "\n",
    "From a model perspective: \n",
    "* Is there a better model we could use?\n",
    "* Could we improve the current model?\n",
    "\n",
    "Hyperparameters vs Parameters \n",
    "\n",
    "Parameters = model find these patterns in data \n",
    "Hyperparameters = settings on a model you can adjust to (potentially) improve it's ability to find patterns. \n",
    "\n",
    "Three ways to adjust hyperparameters: \n",
    "1. By hand\n",
    "2. Randomly with RandomSearchCV\n",
    "3. Exhaustively with GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to get the Hyperparameters\n",
    "\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210",
   "metadata": {},
   "source": [
    "### 5.1 Tuning Hyperparameter by hand \n",
    "\n",
    "Let's make 3 sets, training, validation and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211",
   "metadata": {},
   "source": [
    "<img src = \"./images/hp.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212",
   "metadata": {},
   "source": [
    "<img src=\"./images/concept.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params() # look scikit learn doc they suggest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214",
   "metadata": {},
   "source": [
    "We're going try and adjust: \n",
    "\n",
    "* `max_depth`\n",
    "* `max_features`\n",
    "* `min_samples_leaf`\n",
    "* `min_samples_split`\n",
    "* `n_estimators`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215",
   "metadata": {},
   "outputs": [],
   "source": [
    " def evaluate_preds(y_true, y_preds):\n",
    "     \"\"\"\n",
    "     Performs evaluation comparison on y_true labels vs. y_pred labels on a classification model.\n",
    "     \"\"\"\n",
    "     accuracy = accuracy_score(y_true,y_preds)\n",
    "     precision = precision_score(y_true,y_preds)\n",
    "     recall = recall_score(y_true,y_preds)\n",
    "     f1 = f1_score(y_true,y_preds)\n",
    "     metric_dict = { \"accuracy\": round(accuracy,2),\n",
    "                     \"precision\": round (precision,2),\n",
    "                    \"recall\": round (recall,2), \n",
    "                    \"f1\": round(f1,2)}\n",
    "     print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "     print(f\"Precision: {precision * 100:.2f}\")\n",
    "     print(f\"Recall: {recall * 100:.2f}\")\n",
    "     print(f\"F1 score: {f1 * 100:.2f}\")\n",
    "\n",
    "     return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#Shuffle the data \n",
    "heart_disease_shuffled = heart_disease.sample(frac=1) \n",
    "\n",
    "# Split into X & y\n",
    "\n",
    "X = heart_disease_shuffled.drop(\"target\", axis = 1) \n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# Split the data into train, validation & test sets \n",
    "train_split = round(0.7*len(heart_disease_shuffled)) #70% of data\n",
    "valid_split = round(train_split+0.15 *len(heart_disease_shuffled))\n",
    "X_train,y_train = X[:train_split], y[:train_split]\n",
    "X_valid,y_valid = X[train_split:valid_split], y[train_split:valid_split]\n",
    "X_test,y_test =X[valid_split:], y[valid_split:]\n",
    "\n",
    "len(X_train), len(X_valid), len(X_test)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Make baseline predictions\n",
    "y_preds = clf.predict(X_valid)\n",
    "\n",
    "# Evaluate the classifier on validation set \n",
    "baseline_metrics = evaluate_preds(y_valid,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "# create a second classifier with diffrent hyperparameters \n",
    "clf_2 = RandomForestClassifier(n_estimators=1000)\n",
    "clf_2.fit(X_train,y_train)\n",
    "# Make  predictions with different HP \n",
    "y_preds_2 = clf_2.predict(X_valid)\n",
    "\n",
    "# Evaluate the 2nd classifier on validation set \n",
    "metrics = evaluate_preds(y_valid,y_preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "# create a third classifier with diffrent hyperparameters \n",
    "clf_3 = RandomForestClassifier( n_estimators=500,\n",
    "    max_features=\"sqrt\",\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=10)\n",
    "\n",
    "\n",
    "clf_3.fit(X_train,y_train)\n",
    "# Make  predictions with different HP \n",
    "y_preds_3 = clf_3.predict(X_valid)\n",
    "\n",
    "# Evaluate the 3rd classifier on validation set \n",
    "metrics_2 = evaluate_preds(y_valid,y_preds_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "grid = {\"n_estimators\":[10,100,200,500,1000,1200],\n",
    "        \"max_depth\":[None, 5,10,20,30],\n",
    "        \"max_features\": [\"log2\",\"sqrt\"],\n",
    "        \"min_samples_split\":[2,4,6],\n",
    "        \"min_samples_leaf\":[1,2,4]}\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#Split into X & y\n",
    "X = heart_disease_shuffled.drop(\"target\",axis = 1) \n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "#Split into train and test sets \n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Instantiate RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_jobs = 1 ) # How much of the processor we dedicate to the model\n",
    "\n",
    "#Setup RandomizedSearchCV\n",
    "rs_clf = RandomizedSearchCV(estimator=clf,\n",
    "                            param_distributions=grid,\n",
    "                            n_iter=10, # number of models to try\n",
    "                            cv = 5, #5 fold cross validation \n",
    "                            verbose = 2 # the detail of the output 0-3 \n",
    "                           )\n",
    "# Fit the RSCV version of clf \n",
    "rs_clf.fit(X_train,y_train); # automatically will make a valid set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions  with the best parameters \n",
    "rs_y_preds = rs_clf.predict(X_test) \n",
    "\n",
    "#Evaluate the predictions\n",
    "rs_metrics = evaluate_preds(y_test,rs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid Search is kind of Brute Force Search\n",
    "\n",
    "6*5*2*3*3*5  # amount of diffrent models the last 5 because of CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_2 = {'n_estimators': [ 100, 200, 500],\n",
    " 'max_depth': [None],\n",
    " 'max_features': ['log2', 'sqrt'], # auto is not supported\n",
    " 'min_samples_split': [6],\n",
    " 'min_samples_leaf': [1, 2]} # We reduced the combinations of Hyperparameters based on the result of the RSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227",
   "metadata": {},
   "outputs": [],
   "source": [
    "3*1*2*1*2*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#Split into X & y\n",
    "X = heart_disease_shuffled.drop(\"target\",axis = 1) \n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "#Split into train and test sets \n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Instantiate RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_jobs = 1 ) # How much of the processor we dedicate to the model\n",
    "\n",
    "#Setup GridSearchCV\n",
    "gs_clf = GridSearchCV(estimator=clf,\n",
    "                            param_grid=grid_2,\n",
    "                            cv = 5, #5 fold cross validation \n",
    "                            verbose = 2 # the detail of the output 0-3 \n",
    "                           )\n",
    "# Fit the RSCV version of clf \n",
    "gs_clf.fit(X_train,y_train); # automatically will make a valid set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_y_preds = gs_clf.predict(X_test)\n",
    "\n",
    "# evaluate the predictions\n",
    "gs_metrics = evaluate_preds(y_test,gs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230",
   "metadata": {},
   "source": [
    "Let's compare our diffrent models metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metrics = pd.DataFrame({\"baseline\":baseline_metrics,\n",
    "                               \"clf_2\":metrics_2,\n",
    "                                \"random search\":rs_metrics,\n",
    "                                \"grid search\":gs_metrics})\n",
    "\n",
    "compare_metrics.plot.bar(figsize = (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it depends what should be the focus of the model accuracy or precision or recall or f1 \n",
    "# look at: https://colab.research.google.com/drive/1ISey96a5Ag6z2CvVZKVqTKNWRwZbZl0m\n",
    "# for right train test split for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233",
   "metadata": {},
   "outputs": [],
   "source": [
    "whats_were_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234",
   "metadata": {},
   "source": [
    "## 6. Save and load a trained model\n",
    "\n",
    "Two ways to save and load machine learning models: \n",
    "1. With Python's `pickle` module\n",
    "2. With the `joblib`module\n",
    "\n",
    "**Pickle** Python Object Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Python Object is our model \n",
    "import pickle \n",
    "\n",
    "# Save an existing model to file \n",
    "with open(\"gs_random_forest_model_1.pkl\", \"wb\") as f: #\n",
    "    pickle.dump(gs_clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model\n",
    "with open(\"gs_random_forest_model_1.pkl\", \"rb\") as f:\n",
    "    loaded_pickle_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "pickle_y_preds = loaded_pickle_model.predict(X_test)\n",
    "evaluate_preds(y_test,pickle_y_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238",
   "metadata": {},
   "source": [
    "### Joblib Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load # joblib is more efficient!!!\n",
    "\n",
    "# save model to file \n",
    "\n",
    "dump(gs_clf, filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a saved joblib model \n",
    "\n",
    "loaded_job_model = load(filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib_y_preds = loaded_job_model.predict(X_test)\n",
    "evaluate_preds(y_test,joblib_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242",
   "metadata": {},
   "outputs": [],
   "source": [
    "whats_were_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243",
   "metadata": {},
   "source": [
    "## 7. Putting it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247",
   "metadata": {},
   "source": [
    "<img src=\"images/all.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248",
   "metadata": {},
   "source": [
    "Steps we want to do (all in one cell):\n",
    "1. Fill missing data\n",
    "2. Convert data to numbers\n",
    "3. Build a model on the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data ready\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Setup random seed \n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import data and drop rows with missing labels\n",
    "data = pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "data.dropna(subset=[\"Price\"], inplace = True)\n",
    "\n",
    "# Define diffrent features and transformer pipeline \n",
    "categorical_features = [\"Make\",\"Colour\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy =\"constant\", fill_value=\"missing\" )),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "door_feature =[\"Doors\"]\n",
    "door_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy =\"constant\", fill_value=4 )),\n",
    "    ])\n",
    "\n",
    "numeric_features = [\"Odometer (KM)\"]\n",
    "numeric_transformer = door_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy =\"mean\" )),\n",
    "    ])\n",
    "\n",
    "# Setup the preprocessing steps (fill missing values then convert to numbers \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", categorical_transformer, categorical_features),\n",
    "                 (\"door\",door_transformer,door_feature),\n",
    "                 (\"num\", numeric_transformer, numeric_features)\n",
    "                ])\n",
    "\n",
    "# Creating a preprocessing and modelling pipeline\n",
    "model = Pipeline(steps=[(\"preprocessor\",preprocessor),\n",
    "                        (\"model\",RandomForestRegressor())])\n",
    "#Split the data\n",
    "X = data.drop(\"Price\", axis = 1)\n",
    "y = data[\"Price\"]\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Fit and score the model\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warum Pipeline/Imputer wichtig ist:\n",
    "# Alles was .fit() macht (Imputer, OneHotEncoder, Scaling) \"lernt\" Regeln aus den Daten\n",
    "# z.B. Mittelwert/ häufigste Kategorie/ alle vorhandenen Kategorien.\n",
    "# Wenn du diese Schritte auf dem ganzen X fit-test, fließen Infos aus X_test ins Training → Data Leakage.\n",
    "# Das Modell sieht die Testdaten dann indirekt, weil X_train durch Test-Statistiken transformiert wird.\n",
    "# Pipeline verhindert das automatisch: fit nur auf X_train, transform auf X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251",
   "metadata": {},
   "source": [
    "It's also possible to use `GridSearchCV`of `RandomizedSearchCV`with our `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GSCV with our regression pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\" : [\"mean\", \"median\"], \n",
    "    \"model__n_estimators\": [100,1000],\n",
    "    \"model__max_depth\":[None,5],\n",
    "    \"model__max_features\": [\"sqrt\"],\n",
    "    \"model__min_samples_split\":[2,4]\n",
    "}\n",
    "\n",
    "gs_model = GridSearchCV(model, pipe_grid,cv=5, verbose = 2)\n",
    "gs_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254",
   "metadata": {},
   "outputs": [],
   "source": [
    "whats_were_covering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
